{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDwep1K8Erxl"
   },
   "source": [
    "**Project:** Data Minining Project for  X company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "JzIu-UWIDXHw",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7-ii3uyI8KY"
   },
   "source": [
    "The CRISP-DM Framework\n",
    "\n",
    "\n",
    "The CRISP-DM methodology provides a structured approach to planning a data mining project. It is a robust and well-proven methodology.\n",
    "* Business understanding (BU): Determine Business Objectives, Assess Situation, Determine Data Mining Goals, Produce Project Plan\n",
    "\n",
    "* Data understanding (DU): Collect Initial Data, Describe Data, Explore Data, Verify Data Quality\n",
    "\n",
    "* Data preparation (DP): Select Data, Clean Data, Construct Data, Integrate Data\n",
    "\n",
    "* Modeling (M): Select modeling technique, Generate Test Design, Build Model, Assess Model\n",
    "*  Evaluation (E): Evaluate Results, Review Process, Determine Next Steps\n",
    "*  Deployment (D): Plan Deployment, Plan Monitoring and Maintenance, Produce Final Report, Review Project\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "[What is the CRISP-DM methodology?](https://www.sv-europe.com/crisp-dm-methodology/)\n",
    "\n",
    "[Introduction to CRISP DM Framework for Data Science and Machine Learning](https://www.linkedin.com/pulse/chapter-1-introduction-crisp-dm-framework-data-science-anshul-roy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lo7Ml7tMQOf"
   },
   "source": [
    "**Data Set**\n",
    "### The data is for company X which is trying to control attrition. \n",
    "### There are two sets of data: \"Existing employees\" and \"Employees who have left\". The following attributes are available for every employee.\n",
    "\n",
    "\n",
    "*   Satisfaction Level\n",
    "\n",
    "*   Last evaluation\n",
    "\n",
    "*   Number of projects\n",
    "\n",
    "*   Average monthly hours\n",
    "\n",
    "*   Time spent at the company\n",
    "*   Whether they have had a work accident\n",
    "\n",
    "\n",
    "*  Whether they have had a promotion in the last 5 years\n",
    "\n",
    "\n",
    "*   Departments (column sales)\n",
    "\n",
    "\n",
    "*   Salary\n",
    "\n",
    "\n",
    "*  Whether the employee has left\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjSj2A2sSph_"
   },
   "source": [
    "**Your Role**\n",
    " \n",
    "\n",
    "*   As data science team member X company asked you to answer this two questions.\n",
    "*  What type of employees is leaving? \n",
    "\n",
    "*   Determine which employees are prone to leave next.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajdEVA7LiBUp"
   },
   "source": [
    "Business Understanding\n",
    "\n",
    "---\n",
    "\n",
    "This step mostly focuses on understanding the Business in all the different aspects. It follows the below different steps.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Identify the goal and frame the business problem.\n",
    "* Prepare Analytical Goal i.e. what type of performance metric and loss function to use\n",
    "* Gather information on resource, constraints, assumptions, risks etc\n",
    "* Gather information on resource, constraints, assumptions, risks etc\n",
    "*   Prepare Work Flow Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4MwiCYzj2_u"
   },
   "source": [
    "### Write the main objectives of this project in your words?\n",
    "minimum of 100 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "STyLda45j1Mf",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "main_objectives ='''The main objectives are to determine the characteristics of employees who leave compared to those \n",
    "                    who stay and use the results to predict the current and future employees who are likely to leave based on their \n",
    "                    attributes.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "CuOlxLxKMOLI",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(main_objectives) > 100 \n",
    "### BEGIN HIDDEN TESTS\n",
    "assert len(main_objectives) > 80 \n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyXeNxlCkbaw"
   },
   "source": [
    "### Outline the different data analysis steps you will follow to carry out the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "rC-tl8sUksQq",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dm_outline = '''Conduct exploratory data analysis to make the data for the main analysis and begin with an unsupervised learning method\n",
    "                i.e ignore information on who left and who stayed to determine whether the employees would be distinguished in two main\n",
    "                groups and the component(s) that majorly distinguishes them, this component would be of interest to the company. \n",
    "                I would then conduct a supervised learning using train, validation and test datasets for prediction. I would \n",
    "                compare different classifiers and determine the classifier with the best prediction ability.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "-K1mWuDoksTk",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(dm_outline) > 100 \n",
    "### BEGIN HIDDEN TESTS\n",
    "assert len(dm_outline) > 70 \n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmUDFG1wkzUy"
   },
   "source": [
    "### What metrics will you use to measure the performance of your data analysis model? \n",
    "Write the equations of the metrics here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCNulojKk_BP"
   },
   "source": [
    "e.g. Precision = $\\frac{TP}{(TP + FP)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Recall=\\frac{TP}{TP+FN}$$\n",
    "$$Precision=\\frac{TP}{TP+FP}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLS2YHoRk_EK"
   },
   "source": [
    "Why do you choose these metrics? minimum of 100 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "LSynT14KlPSJ",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "why_metrics = '''These metrics would help in analyzing how well the class of interest is being handled by the model. \n",
    "High recall and high precision would mean the class is being well handled by the model while low precision would \n",
    "mean the class is poorly handled.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "yr-Mk0E8lPVJ",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(why_metrics) > 100 \n",
    "### BEGIN HIDDEN TESTS\n",
    "assert len(why_metrics) > 80 \n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAo19Ip6lUtm"
   },
   "source": [
    "### How would you know if your data analysis work is a success or not?\n",
    "minimum of 100 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "HESsiXW5llX-",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "how_success = '''High recall and high precision would mean the class is being well handled by the model while low precision would \n",
    "mean the class is poorly handled. High recall and low precision would mean the class can be well detected but includes values \n",
    "from other classes. Low recall and high precision means the class is not being well detected but very reliable when it \n",
    "is detected\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "FdUoiMIOlmXq",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(how_success) > 100 \n",
    "### BEGIN HIDDEN TESTS\n",
    "assert len(how_success) > 80 \n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQE6dqo6l1TZ"
   },
   "source": [
    "## What kind of challenges do you expect in your analysis?\n",
    "List at least 3 challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "WrAhBQhQl8Lh",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "challenge_text = '''There would be a problem choosing and tuning the best classification model, the factors \n",
    "       distinguishing the two sets of employees may not be apparent leading to a black-box model, some attributes \n",
    "       may contain unreliable data such as level of satisfaction due to the relationship between employer and \n",
    "       employee\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "EedHa-Pll8X7",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(challenge_text) > 100 \n",
    "### BEGIN HIDDEN TESTS\n",
    "assert len(how_success) > 80 \n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcJ8M6uWDeSE"
   },
   "source": [
    "<h2>Using the processed twitter data from yesterday's challenge</h2>.\n",
    "\n",
    "\n",
    "- Form a new data frame (named `cleanTweet`), containing columns $\\textbf{clean-text}$ and $\\textbf{polarity}$.\n",
    "\n",
    "- Write a function `text_category` that takes a value `p` and returns, depending on the value of p, a string `'positive'`, `'negative'` or `'neutral'`.\n",
    "\n",
    "- Apply this function (`text_category`) on the $\\textbf{polarity}$ column of `cleanTweet` in 1 above to form a new column called $\\textbf{score}$ in `cleanTweet`.\n",
    "\n",
    "- Visualize The $\\textbf{score}$ column using piechart and barchart\n",
    "\n",
    "<h5>Now we want to build a classification model on the clean tweet following the steps below:</h5>\n",
    "\n",
    "* Remove rows from `cleanTweet` where $\\textbf{polarity}$ $= 0$ (i.e where $\\textbf{score}$ = Neutral) and reset the frame index.\n",
    "* Construct a column $\\textbf{scoremap}$ Use the mapping {'positive':1, 'negative':0} on the $\\textbf{score}$ column\n",
    "* Create feature and target variables `(X,y)` from $\\textbf{clean-text}$ and $\\textbf{scoremap}$ columns respectively.\n",
    "* Use `train_test_split` function to construct `(X_train, y_train)` and `(X_test, y_test)` from `(X,y)`\n",
    "\n",
    "* Build an `SGDClassifier` model from the vectorize train text data. Use `CountVectorizer()` with a $\\textit{trigram}$ parameter.\n",
    "\n",
    "* Evaluate your model on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "85WxmGNGDcBY",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/cleaned_fintech_data.csv\")\n",
    "#dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#create new dataframe cleanTweet\n",
    "cleanTweet=dataset[['clean_text','polarity']]\n",
    "#cleanTweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "c:\\users\\stella\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#convert datatype of polarity to numeric.\n",
    "#cleanTweet = cleanTweet.drop([cleanTweet.index[2810]])\n",
    "#cleanTweet =cleanTweet[pd.to_numeric(cleanTweet['polarity'], errors='coerce')]\n",
    "cleanTweet['polarity']=pd.to_numeric(cleanTweet['polarity'],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Create text_category function\n",
    "def text_category(p):\n",
    "    if p==0:\n",
    "        return 'neutral'\n",
    "    elif p>0:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "c:\\users\\stella\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#Apply text_category function\n",
    "cleanTweet['score'] = cleanTweet['polarity'].apply(text_category)\n",
    "#cleanTweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "c:\\users\\stella\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py:4315: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#Drop neutral scores\n",
    "cleanTweet.drop(cleanTweet.loc[cleanTweet['score']=='neutral'].index, inplace=True)\n",
    "cleanTweet=cleanTweet.reset_index(drop=True)\n",
    "#cleanTweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#create score map\n",
    "score_dict={'positive':1,'negative':0}\n",
    "cleanTweet['scoremap']= cleanTweet['score'].map(score_dict)\n",
    "#cleanTweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(cleanTweet['clean_text'], cleanTweet['scoremap'], test_size = 0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#vectorize \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(3, 3))\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X_train_cv, y_train)\n",
    "predictions = clf.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Confusion Matrix :\n",
      "[[169  10]\n",
      " [  2 517]]\n",
      "Accuracy Score : 0.9828080229226361\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.97       179\n",
      "           1       0.98      1.00      0.99       519\n",
      "\n",
      "    accuracy                           0.98       698\n",
      "   macro avg       0.98      0.97      0.98       698\n",
      "weighted avg       0.98      0.98      0.98       698\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#Analysis of the performance metrics for each category\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix,classification_report\n",
    "results = confusion_matrix(y_test, predictions) \n",
    "  \n",
    "print ('Confusion Matrix :')\n",
    "print(results) \n",
    "print ('Accuracy Score :',accuracy_score(y_test, predictions)) \n",
    "print ('Report : ')\n",
    "print (classification_report(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Challenge_ Day2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}