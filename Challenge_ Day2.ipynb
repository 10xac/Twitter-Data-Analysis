{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Challenge_ Day2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python385jvsc74a57bd0c30745a67ab3077685183c6616f760120387d2e186c7e6a99675e86d34cdfa12","display_name":"Python 3.8.5 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zDwep1K8Erxl"},"source":["**Project:** Data Minining Project for  X company"]},{"cell_type":"markdown","metadata":{"id":"d7-ii3uyI8KY"},"source":["The CRISP-DM Framework\n","\n","\n","The CRISP-DM methodology provides a structured approach to planning a data mining project. It is a robust and well-proven methodology.\n","* Business understanding (BU): Determine Business Objectives, Assess Situation, Determine Data Mining Goals, Produce Project Plan\n","\n","* Data understanding (DU): Collect Initial Data, Describe Data, Explore Data, Verify Data Quality\n","\n","* Data preparation (DP): Select Data, Clean Data, Construct Data, Integrate Data\n","\n","* Modeling (M): Select modeling technique, Generate Test Design, Build Model, Assess Model\n","*  Evaluation (E): Evaluate Results, Review Process, Determine Next Steps\n","*  Deployment (D): Plan Deployment, Plan Monitoring and Maintenance, Produce Final Report, Review Project\n","\n","\n","References:\n","\n","[What is the CRISP-DM methodology?](https://www.sv-europe.com/crisp-dm-methodology/)\n","\n","[Introduction to CRISP DM Framework for Data Science and Machine Learning](https://www.linkedin.com/pulse/chapter-1-introduction-crisp-dm-framework-data-science-anshul-roy/)"]},{"cell_type":"markdown","metadata":{"id":"5lo7Ml7tMQOf"},"source":["**Data Set**\n","### The data is for company X which is trying to control attrition. \n","### There are two sets of data: \"Existing employees\" and \"Employees who have left\". The following attributes are available for every employee.\n","\n","\n","*   Satisfaction Level\n","\n","*   Last evaluation\n","\n","*   Number of projects\n","\n","*   Average monthly hours\n","\n","*   Time spent at the company\n","*   Whether they have had a work accident\n","\n","\n","*  Whether they have had a promotion in the last 5 years\n","\n","\n","*   Departments (column sales)\n","\n","\n","*   Salary\n","\n","\n","*  Whether the employee has left\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sjSj2A2sSph_"},"source":["**Your Role**\n"," \n","\n","*   As data science team member X company asked you to answer this two questions.\n","*  What type of employees is leaving? \n","\n","*   Determine which employees are prone to leave next.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ajdEVA7LiBUp"},"source":["Business Understanding\n","\n","---\n","\n","This step mostly focuses on understanding the Business in all the different aspects. It follows the below different steps.\n","\n","\n","\n","\n","* Identify the goal and frame the business problem.\n","* Prepare Analytical Goal i.e. what type of performance metric and loss function to use\n","* Gather information on resource, constraints, assumptions, risks etc\n","* Gather information on resource, constraints, assumptions, risks etc\n","*   Prepare Work Flow Chart"]},{"cell_type":"markdown","metadata":{"id":"J4MwiCYzj2_u"},"source":["### Write the main objectives of this project in your words?\n","minimum of 100 characters"]},{"cell_type":"code","metadata":{"id":"STyLda45j1Mf"},"source":["main_objectives ='''\n","The main objectives of this project are:\n","  * to keep employees form leaving X company\n","  * to increase employees' continuity in X company\n","  * to identify the factors which affect employee's stay in X company so that decisions could be taken to     control attrition\n","'''"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"CuOlxLxKMOLI"},"source":["assert len(main_objectives) > 100 \n","### BEGIN HIDDEN TESTS\n","assert len(main_objectives) > 80 \n","### END HIDDEN TESTS"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NyXeNxlCkbaw"},"source":["### Outline the different data analysis steps you will follow to carry out the project"]},{"cell_type":"code","metadata":{"id":"rC-tl8sUksQq"},"source":["dm_outline = '''\n","n order to carry out data analysis for this project, I will follow these steps.\n","  1. Understand the business problem\n","  2. Extract and understand the datasets\n","  3. Handle missing values and outliers\n","  4. Transform the data, if necessary\n","  5. Perform feature engineering tasks\n","  6. Split the data into training, validation and testing sets\n","  7. Build classifier models\n","  8. Measure the models' results and choose the best one\n","  9. Generate recommendation reports\n","'''"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"-K1mWuDoksTk"},"source":["assert len(dm_outline) > 100 \n","### BEGIN HIDDEN TESTS\n","assert len(dm_outline) > 70 \n","### END HIDDEN TESTS"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pmUDFG1wkzUy"},"source":["### What metrics will you use to measure the performance of your data analysis model? \n","Write the equations of the metrics here"]},{"cell_type":"markdown","metadata":{"id":"KCNulojKk_BP"},"source":["e.g. Precision = $\\frac{TP}{(TP + FP)}$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vLS2YHoRk_EK"},"source":["Why do you choose these metrics? minimum of 100 characters"]},{"cell_type":"code","metadata":{"id":"LSynT14KlPSJ"},"source":["why_metrics = '''Add your answer text here\n","you can create python string using (') or (\") or 3('), like the text here. The 3(') string can be used \n","to write paragraphs, comments in the beginning of functions, etc.. Your answer to the above question \n","should replace this text.\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yr-Mk0E8lPVJ"},"source":["assert len(why_metrics) > 100 \n","### BEGIN HIDDEN TESTS\n","assert len(why_metrics) > 80 \n","### END HIDDEN TESTS"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aAo19Ip6lUtm"},"source":["### How would you know if your data analysis work is a success or not?\n","minimum of 100 characters"]},{"cell_type":"code","metadata":{"id":"HESsiXW5llX-"},"source":["how_success = '''\n","I would know that my data analysis work is a success if my audience,X company, accepts the results that arise from it.\n","In other words, it would be a success if my data analysis model could successfully predict atleast\n","the majority(a precentage decided by X company) of the employeees who leave X company.\n","'''"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"FdUoiMIOlmXq"},"source":["assert len(how_success) > 100 \n","### BEGIN HIDDEN TESTS\n","assert len(how_success) > 80 \n","### END HIDDEN TESTS"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DQE6dqo6l1TZ"},"source":["## What kind of challenges do you expect in your analysis?\n","List at least 3 challenges"]},{"cell_type":"code","metadata":{"id":"WrAhBQhQl8Lh"},"source":["challenge_text = '''\n","The main challenges that I expect in my data analysis process are:\n","  1. Small amount of data\n","  2. Missing values that are hard to fill which can reduce my success\n","  3. Undiscovered but relevant features that are hard to engineer\n","  4. Model overfitting\n","'''"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"EedHa-Pll8X7"},"source":["assert len(challenge_text) > 100 \n","### BEGIN HIDDEN TESTS\n","assert len(how_success) > 80 \n","### END HIDDEN TESTS"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZcJ8M6uWDeSE"},"source":["<h2>Using the processed twitter data from yesterday's challenge</h2>.\n","\n","\n","- Form a new data frame (named `cleanTweet`), containing columns $\\textbf{clean-text}$ and $\\textbf{polarity}$.\n","\n","- Write a function `text_category` that takes a value `p` and returns, depending on the value of p, a string `'positive'`, `'negative'` or `'neutral'`.\n","\n","- Apply this function (`text_category`) on the $\\textbf{polarity}$ column of `cleanTweet` in 1 above to form a new column called $\\textbf{score}$ in `cleanTweet`.\n","\n","- Visualize The $\\textbf{score}$ column using piechart and barchart\n","\n","<h5>Now we want to build a classification model on the clean tweet following the steps below:</h5>\n","\n","* Remove rows from `cleanTweet` where $\\textbf{polarity}$ $= 0$ (i.e where $\\textbf{score}$ = Neutral) and reset the frame index.\n","* Construct a column $\\textbf{scoremap}$ Use the mapping {'positive':1, 'negative':0} on the $\\textbf{score}$ column\n","* Create feature and target variables `(X,y)` from $\\textbf{clean-text}$ and $\\textbf{scoremap}$ columns respectively.\n","* Use `train_test_split` function to construct `(X_train, y_train)` and `(X_test, y_test)` from `(X,y)`\n","\n","* Build an `SGDClassifier` model from the vectorize train text data. Use `CountVectorizer()` with a $\\textit{trigram}$ parameter.\n","\n","* Evaluate your model on the test data.\n"]},{"source":["<h3>Extract tweets dataframe<h3>"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","metadata":{"id":"85WxmGNGDcBY"},"source":["import extract_dataframe as exdf \n","\n","_, tweet_list = exdf.read_json(\"./data/covid19.json\")\n","tweets_df_extractor = exdf.TweetDfExtractor(tweet_list)\n","tweets_df = tweets_df_extractor.get_tweet_df()\n","tweets_df"],"execution_count":15,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'retweeted_status'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-82696feb5a74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/covid19.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtweets_df_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTweetDfExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtweets_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweets_df_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tweet_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtweets_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/Twitter-Data-Analysis/extract_dataframe.py\u001b[0m in \u001b[0;36mget_tweet_df\u001b[0;34m(self, save)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mcreated_at\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_created_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_full_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mpolarity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubjectivity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_sentiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/Twitter-Data-Analysis/extract_dataframe.py\u001b[0m in \u001b[0;36mfind_full_text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_full_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         text = [x['retweeted_status']['extended_tweet']['full_text']\n\u001b[0m\u001b[1;32m     46\u001b[0m                 for x in self.tweets_list]\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/Documents/Twitter-Data-Analysis/extract_dataframe.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_full_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         text = [x['retweeted_status']['extended_tweet']['full_text']\n\u001b[0m\u001b[1;32m     46\u001b[0m                 for x in self.tweets_list]\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'retweeted_status'"]}]}]}