{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = pd.read_json(\"data/covid19.json\",lines=True)\n",
    "dataset.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.utils import SaveLoad\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from re import sub\n",
    "import pyLDAvis.gensim\n",
    "from collections import Counter\n",
    "from gensim.matutils import corpus2csc, sparse2full, corpus2dense\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.utils import resample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Functions for data cleaning\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def removePunc(myWord):\n",
    "    \"\"\"Function to remove punctuation from string inputs\"\"\"\n",
    "    if myWord is None:\n",
    "        return myWord\n",
    "    else:\n",
    "        return sub('[.:;()/!&-*@$,?^\\d+]','',myWord)\n",
    "        \n",
    "def removeAscii(myWord):\n",
    "    \"\"\"Function to remove ascii from string input\"\"\"\n",
    "    if myWord is None:\n",
    "        return myWord\n",
    "    else:\n",
    "        return str(sub(r'[^\\x00-\\x7F]+','', myWord.decode('utf-8').strip()))\n",
    "\n",
    "def lemmatize(myWord):\n",
    "    \"\"\"Function to lemmatize words\"\"\"\n",
    "    if myWord is None:\n",
    "        return myWord\n",
    "    else:\n",
    "        return str(wnl.lemmatize(myWord))\n",
    "\n",
    "def removeStopWords(myWord):\n",
    "    \"\"\"Function to remove stop words\"\"\"\n",
    "    if myWord is None:\n",
    "        return myWord\n",
    "    if myWord not in str(stopwords.words('english')):\n",
    "        return myWord\n",
    "\n",
    "def removeLinkUser(myWord):\n",
    "    \"\"\"Function to remove web addresses and twitter handles\"\"\"\n",
    "    if not myWord.startswith('@') and not myWord.startswith('http'):\n",
    "        return myWord\n",
    "\n",
    "def prepText(myWord):\n",
    "    \"\"\"Final text pre-processing function\"\"\"\n",
    "    return removeStopWords(\n",
    "        lemmatize(\n",
    "            removeAscii(\n",
    "                removePunc(\n",
    "                    removeLinkUser(\n",
    "                        myWord.lower()\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def filterTweetList(tweetList):\n",
    "    \"\"\"Remove stop words, lemmatize, and clean all tweets\"\"\"\n",
    "    return [[prepText(word) for word\n",
    "                in tweet.split()\n",
    "                    if prepText(word) is not None]\n",
    "                for tweet in tweetList]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleanTweetList = filterTweetList(dataset['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>...</th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>quoted_status</th>\n",
       "      <th>quoted_status_permalink</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>extended_tweet</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>withheld_in_countries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-06-18 17:55:49+00:00</td>\n",
       "      <td>1405947374003015684</td>\n",
       "      <td>1405947374003015680</td>\n",
       "      <td>RT @TelGlobalHealth: ðŸš¨Africa is \"in the midst ...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-06-18 17:55:49.858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-06-18 17:55:59+00:00</td>\n",
       "      <td>1405947412364075010</td>\n",
       "      <td>1405947412364075008</td>\n",
       "      <td>RT @globalhlthtwit: Dr Moeti is head of WHO in...</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-06-18 17:55:59.004</td>\n",
       "      <td>1.405895e+18</td>\n",
       "      <td>1.405895e+18</td>\n",
       "      <td>{'created_at': 'Fri Jun 18 14:27:42 +0000 2021...</td>\n",
       "      <td>{'url': 'https://t.co/sOgIroihOc', 'expanded':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at                   id               id_str  \\\n",
       "0 2021-06-18 17:55:49+00:00  1405947374003015684  1405947374003015680   \n",
       "1 2021-06-18 17:55:59+00:00  1405947412364075010  1405947412364075008   \n",
       "\n",
       "                                                text  \\\n",
       "0  RT @TelGlobalHealth: ðŸš¨Africa is \"in the midst ...   \n",
       "1  RT @globalhlthtwit: Dr Moeti is head of WHO in...   \n",
       "\n",
       "                                              source  truncated  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...      False   \n",
       "1  <a href=\"https://mobile.twitter.com\" rel=\"nofo...      False   \n",
       "\n",
       "   in_reply_to_status_id  in_reply_to_status_id_str  in_reply_to_user_id  \\\n",
       "0                    NaN                        NaN                  NaN   \n",
       "1                    NaN                        NaN                  NaN   \n",
       "\n",
       "   in_reply_to_user_id_str  ...            timestamp_ms quoted_status_id  \\\n",
       "0                      NaN  ... 2021-06-18 17:55:49.858              NaN   \n",
       "1                      NaN  ... 2021-06-18 17:55:59.004     1.405895e+18   \n",
       "\n",
       "   quoted_status_id_str                                      quoted_status  \\\n",
       "0                   NaN                                                NaN   \n",
       "1          1.405895e+18  {'created_at': 'Fri Jun 18 14:27:42 +0000 2021...   \n",
       "\n",
       "                             quoted_status_permalink  display_text_range  \\\n",
       "0                                                NaN                 NaN   \n",
       "1  {'url': 'https://t.co/sOgIroihOc', 'expanded':...                 NaN   \n",
       "\n",
       "  extended_tweet  possibly_sensitive  extended_entities  withheld_in_countries  \n",
       "0            NaN                 NaN                NaN                    NaN  \n",
       "1            NaN                 NaN                NaN                    NaN  \n",
       "\n",
       "[2 rows x 37 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanTweetList = filterTweetList(dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bb25f027dc40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mre\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpus2csc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse2full\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus2dense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.utils import SaveLoad\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from re import sub\n",
    "import pyLDAvis.gensim\n",
    "from collections import Counter\n",
    "from gensim.matutils import corpus2csc, sparse2full, corpus2dense\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for data cleaning\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def removePunc(myWord):\n",
    "    \"\"\"Function to remove punctuation from string inputs\"\"\"\n",
    "    if myWord is None:\n",
    "        return myWord\n",
    "    else:\n",
    "        return sub('[.:;()/!&-*@$,?^\\d+]','',myWord)\n",
    "        \n",
    "def removeAscii(myWord):\n",
    "    \"\"\"Function to remove ascii from string input\"\"\"\n",
    "    if myWord is None:\n",
    "        return myWord\n",
    "    else:\n",
    "        return str(sub(r'[^\\x00-\\x7F]+','', myWord.decode('utf-8').strip()))\n",
    "\n",
    "def lemmatize(myWord):\n",
    "    \"\"\"Function to lemmatize words\"\"\"\n",
    "    if myWord is None:\n",
    "        return myWord\n",
    "    else:\n",
    "        return str(wnl.lemmatize(myWord))\n",
    "\n",
    "def removeStopWords(myWord):\n",
    "    \"\"\"Function to remove stop words\"\"\"\n",
    "    if myWord is None:\n",
    "        return myWord\n",
    "    if myWord not in str(stopwords.words('english')):\n",
    "        return myWord\n",
    "\n",
    "def removeLinkUser(myWord):\n",
    "    \"\"\"Function to remove web addresses and twitter handles\"\"\"\n",
    "    if not myWord.startswith('@') and not myWord.startswith('http'):\n",
    "        return myWord\n",
    "\n",
    "def prepText(myWord):\n",
    "    \"\"\"Final text pre-processing function\"\"\"\n",
    "    return removeStopWords(\n",
    "        lemmatize(\n",
    "            removeAscii(\n",
    "                removePunc(\n",
    "                    removeLinkUser(\n",
    "                        myWord.lower()\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterTweetList(tweetList):\n",
    "    \"\"\"Remove stop words, lemmatize, and clean all tweets\"\"\"\n",
    "    return [[prepText(word) for word\n",
    "                in tweet.split()\n",
    "                    if prepText(word) is not None]\n",
    "                for tweet in tweetList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanTweetList = filterTweetList(dataset['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}