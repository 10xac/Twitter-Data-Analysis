{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d45cabe9",
   "metadata": {},
   "source": [
    "Objective\n",
    "Social Media Tweet Analysis on Twitter Dataset\n",
    "Topic Modeling on Twitter Dataset\n",
    "Reference for Topic modeling\n",
    "\n",
    "Sentiment analysis on Twitter Dataset\n",
    "\n",
    "Business understanding\n",
    "Topic modeling\n",
    "Topic modeling is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of texts. It is an unsupervised approach used for finding and observing the bunch of words (called ‚Äútopics‚Äù) in large clusters of texts. Topic models are built around the idea that the semantics of our document are actually being governed by some hidden, or ‚Äúlatent,‚Äù variables that we are not observing.\n",
    "\n",
    "Our task here is to discover abstract topics from tweets.\n",
    "Sentiment analysis\n",
    "It is used in social media monitoring, allowing businesses to gain insights about how customers feel about certain topics, and detect urgent issues in real time before they spiral out of control.\n",
    "Our task here is to classify a tweet as a positive or negative tweet sentiment wise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060e308e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mextract_dataframe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_json,TweetDfExtractor\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclean_tweets_dataframe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Clean_Tweets\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m~/Twitter-Data-Analysis/extract_dataframe.py:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ndarray\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_json\u001b[39m(json_file: \u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m\u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    json file reader to open and read json files into a list\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    length of the json file and a list of json\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "from extract_dataframe import read_json,TweetDfExtractor\n",
    "from clean_tweets_dataframe import Clean_Tweets\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn. metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def text_catagory(p):\n",
    "    if p > 0:\n",
    "        return 'positive'\n",
    "    elif p < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "    \n",
    "file_name = 'processed_tweet_data.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "# see polarity\n",
    "df['polarity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "555344eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _, tweet_list \u001b[38;5;241m=\u001b[39m \u001b[43mread_json\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/Economic_Twitter_Data.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m tweet \u001b[38;5;241m=\u001b[39m TweetDfExtractor(tweet_list)\n\u001b[1;32m      3\u001b[0m tweet_df \u001b[38;5;241m=\u001b[39m tweet\u001b[38;5;241m.\u001b[39mget_tweet_df()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_json' is not defined"
     ]
    }
   ],
   "source": [
    "_, tweet_list = read_json(\"data/Economic_Twitter_Data.json\")\n",
    "tweet = TweetDfExtractor(tweet_list)\n",
    "tweet_df = tweet.get_tweet_df() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51ffefc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Clean_Tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cleaner\u001b[38;5;241m=\u001b[39m\u001b[43mClean_Tweets\u001b[49m()\n\u001b[1;32m      2\u001b[0m cleaned_dataframe\u001b[38;5;241m=\u001b[39m cleaner\u001b[38;5;241m.\u001b[39mdrop_duplicate(df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Clean_Tweets' is not defined"
     ]
    }
   ],
   "source": [
    "cleaner=Clean_Tweets()\n",
    "cleaned_dataframe= cleaner.drop_duplicate(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d9409b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcleaned_dataframe\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cleaned_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "cleaned_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11769c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataframe=cleaner.convert_to_numbers(cleaned_dataframe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f5c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataframe=cleaner.remove_non_english_tweets(cleaned_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a91be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataframe = df[['original_text', 'polarity']].rename({'original_text': 'clean_text'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff117b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleaned_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf76f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text_category on the  ùê©ùê®ùê•ùêöùê´ùê¢ùê≠ùê≤  column of cleanTweet, form a new column called  ùê¨ùêúùê®ùê´ùêû\n",
    "cleaned_dataframe['score'] = cleaned_dataframe[\"polarity\"].map(text_catagory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040e60bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d72351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize using BarChart\n",
    "fig,axis=plt.subplots(figsize=(8,6))\n",
    "cleaned_dataframe.groupby('score')['clean_text'].count().plot.bar(ax=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fa746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize using PieChart\n",
    "fig,axis=plt.subplots(figsize=(8,6))\n",
    "cleaned_dataframe.groupby('score')['clean_text'].count().plot.pie(ax=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bec573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows from cleanTweet where  ùê©ùê®ùê•ùêöùê´ùê¢ùê≠ùê≤   =0  (i.e where  ùê¨ùêúùê®ùê´ùêû  = Neutral)\n",
    "cleanTweet = cleaned_dataframe[cleaned_dataframe['polarity'] != 0]\n",
    "\n",
    "# reset the frame index.\n",
    "cleaned_dataframe.reset_index(drop=True, inplace=True)\n",
    "cleaned_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a55134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a column  ùê¨ùêúùê®ùê´ùêûùê¶ùêöùê©  Use the mapping {'positive':1, 'negative':0} on the  ùê¨ùêúùê®ùê´ùêû  column\n",
    "def get_score(value):\n",
    "    return 1 if (value == 'positive') else 0\n",
    "\n",
    "cleaned_dataframe['scoremap'] = cleaned_dataframe['score'].map(get_score)\n",
    "cleaned_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a838a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature and target variables (X,y) from  ùêúùê•ùêûùêöùêß-ùê≠ùêûùê±ùê≠  and  ùê¨ùêúùê®ùê´ùêûùê¶ùêöùê©  columns respectively.\n",
    "X = cleaned_dataframe['clean_text']\n",
    "y = cleaned_dataframe['scoremap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313875e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split function to construct (X_train, y_train) and (X_test, y_test) from (X,y)\n",
    "# i dont' know what it is actually doing\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ba060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an SGDClassifier model from the vectorize train text data. Use CountVectorizer() with a  trigram  parameter.\n",
    "# pipeline = Pipeline([\n",
    "#     ('vector',  CountVectorizer(analyzer='word', ngram_range=(3,3))),\n",
    "#     ('tfitf', TfidfTransformer()),\n",
    "#     ('clf', SGDClassifier(max_iter=1000))\n",
    "# ])\n",
    "# X_train_count = pipeline.fit(x_train, y_train)\n",
    "# can't run this for y_train\n",
    "# y_predection = X_train_count.predict(x_test)\n",
    "cv = CountVectorizer(ngram_range=(3, 3))\n",
    "X_train_cv = cv.fit_transform(x_train)\n",
    "X_test_cv = cv.transform(x_test)\n",
    "X_train_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3379eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your model on the test data.\n",
    "clf = SGDClassifier()\n",
    "clf.fit(X_train_cv, y_train)\n",
    "predictions = clf.predict(X_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf475d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = confusion_matrix(y_test, predictions) \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8829e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "accuracy_score(y_test, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7cfef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0d2b06",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.79      1.00      0.88      4528\n",
    "           1       0.96      0.24      0.39      1629\n",
    "\n",
    "    accuracy                           0.80      6157\n",
    "   macro avg       0.87      0.62      0.63      6157\n",
    "weighted avg       0.83      0.80      0.75      6157\n",
    "\n",
    "Topic modeling is a machine learning technique that automatically analyzes text data to determine cluster words for a set of documents.\n",
    "\n",
    "unsupervised machine learning because it doesn‚Äôt require a predefined list of tags or training data that‚Äôs been previously classified by humans.\n",
    "doesn‚Äôt require training, it‚Äôs a quick and easy way to start analyzing your data.\n",
    "Data Understanding\n",
    "Loading necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db38d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import STOPWORDS,WordCloud\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import string\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adeaff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "157dfa58",
   "metadata": {},
   "source": [
    "Data acquisition\n",
    "\n",
    "For this example we have two option for data acquisition:\n",
    "\n",
    "You can download Twitter dataset directly from Twitter\n",
    "By registering as a developer using this link Here\n",
    "\n",
    "Or you can use downloaded data found at Week0/data/cleaned_fintech_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04d6d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_df=pd.read_csv('cleaned_fintech_data.csv')\n",
    "cleaned_dataframe.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b22f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ca3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataframe = df[['original_text', 'polarity']].rename({'original_text': 'clean_text'}, axis=1)\n",
    "cleaned_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db9ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text_category on the  ùê©ùê®ùê•ùêöùê´ùê¢ùê≠ùê≤  column of cleanTweet, form a new column called  ùê¨ùêúùê®ùê´ùêû\n",
    "cleaned_dataframe['score'] = cleaned_dataframe[\"polarity\"].map(text_catagory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fd61ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows from cleanTweet where  ùê©ùê®ùê•ùêöùê´ùê¢ùê≠ùê≤   =0  (i.e where  ùê¨ùêúùê®ùê´ùêû  = Neutral)\n",
    "cleaned_dataframe = cleaned_dataframe[cleaned_dataframe['polarity'] != 0]\n",
    "\n",
    "# reset the frame index.\n",
    "cleanTweet.reset_index(drop=True, inplace=True)\n",
    "cleanTweet.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e63316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a column  ùê¨ùêúùê®ùê´ùêûùê¶ùêöùê©  Use the mapping {'positive':1, 'negative':0} on the  ùê¨ùêúùê®ùê´ùêû  column\n",
    "def get_score(value):\n",
    "    return 1 if (value == 'positive') else 0\n",
    "\n",
    "cleaned_dataframe['scoremap'] = cleaned_dataframe['score'].map(get_score)\n",
    "cleaned_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63fe162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData:\n",
    "  def __init__(self,df):\n",
    "    self.df=df\n",
    "    \n",
    "  def preprocess_data(self):\n",
    "    #cleanTweet = self.df.loc[self.df['lang'] ==\"en\"]\n",
    "\n",
    "    \n",
    "    #text Preprocessing\n",
    "    cleaned_dataframe['clean_text']=cleaned_dataframe['clean_text'].astype(str)\n",
    "    cleaned_dataframe['clean_text'] = cleaned_dataframe['clean_text'].apply(lambda x: x.lower())\n",
    "    cleaned_dataframe['clean_text']= cleaned_dataframe['clean_text'].apply(lambda x: x.translate(str.maketrans(' ', ' ', string.punctuation)))\n",
    "    \n",
    "    #Converting tweets to list of words For feature engineering\n",
    "    sentence_list = [tweet for tweet in cleaned_dataframe['clean_text']]\n",
    "    word_list = [sent.split() for sent in sentence_list]\n",
    "\n",
    "    #Create dictionary which contains Id and word \n",
    "    word_to_id = corpora.Dictionary(word_list)\n",
    "    corpus_1= [word_to_id.doc2bow(tweet) for tweet in word_list]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return word_list, word_to_id, corpus_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb9d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "PrepareData_obj=PrepareData(cleaned_dataframe)\n",
    "word_list ,id2word,corpus=PrepareData_obj.preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490d7535",
   "metadata": {},
   "source": [
    "Topic Modeling using Latent Dirichlet Allocation\n",
    "based on the distributional hypothesis, (i.e. similar topics make use of similar words) and the statistical mixture hypothesis (i.e. documents talk about several topics) for which a statistical distribution can be determined.\n",
    "\n",
    "The purpose of LDA is mapping each teweets in our corpus to a set of topics which covers a good deal of the words in the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fc8d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d51a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091b720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "\n",
    "#It's a measure of how good the model is. The lower the better. Perplexity is a negative value\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n",
    "doc_lda = lda_model[corpus]\n",
    "\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=word_list, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\n Ldamodel Coherence Score/Accuracy on Tweets: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcacb690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "LDAvis_prepared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
